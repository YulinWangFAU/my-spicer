# -*- coding: utf-8 -*-
"""
SPICER training (slice-level, aligned with paper & author's code)
- ä¿ç•™ä½ çš„æ•´ä½“ç»“æ„ã€EarlyStopping å’Œ TensorBoard
- å¢å¼ºï¼šå¯ç»­è®­(æ¢å¤ optimizer/scheduler/æ—¥å¿—/éšæœºæ•°)ã€æ•è· SIGTERMã€ä¿å­˜ init_model.pth
- ä¿®æ­£ï¼šval() çš„æ ·æœ¬è§£åŒ…é¡ºåºã€ACS ä½¿ç”¨å®é™…åŠ é€Ÿå› å­
"""
import os, sys, argparse, random, signal, shutil
os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'
import numpy as np
import torch
from torch.nn import functional as F
import matplotlib.pyplot as plt
from datetime import datetime
from tqdm import tqdm
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from networks.SPICER_fastmri_network import SPNet
from dataset.pmri_fastmri_brain_lunwen import RealMeasurement
from dataset.pmri_fastmri_brain import fmult
from utils.util import *
from utils.measures import *
from utils.loss_functions import gradient_loss, spicer_loss
import pandas as pd
import torch.optim as optim

# ----------------- è§£æå‚æ•°ï¼ˆæ–°å¢ resume é€‰é¡¹ï¼‰ -----------------
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=200, help='Number of training epochs')
    parser.add_argument('--patience', type=int, default=20, help='Early stopping patience')
    parser.add_argument('--acceleration', type=int, default=4, help='undersampling factor (e.g., 4 or 8)')
    # ç»­è®­ç›¸å…³ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('--resume', action='store_true', help='resume training from checkpoint')
    parser.add_argument('--resume_path', type=str, default='', help='path to a checkpoint_xxx.pth')
    parser.add_argument('--resume_last_dir', type=str, default='', help='directory that contains checkpoint_last.pth')
    return parser.parse_args()

args = parse_args()
epoch_number = args.epochs
patience = args.patience
ACCELERATION = int(args.acceleration)

# ----------------- HPC ç¯å¢ƒ & è®¾å¤‡ -----------------
user = os.environ.get("USER", "unknown_user")
TMPDIR = os.environ.get("TMPDIR", f"/tmp/{user}")
HOME = os.path.expanduser("~")
os.environ['CUPY_CACHE_DIR'] = os.path.join(TMPDIR, "cupy")
os.environ['NUMBA_CACHE_DIR'] = os.path.join(TMPDIR, "numba")
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.cuda.empty_cache()

local_rank = int(os.environ.get("SLURM_PROCID", 0))
device = f'cuda:{local_rank % max(1, torch.cuda.device_count())}' if torch.cuda.is_available() else 'cpu'
print(f"ğŸ§  Using device: {device}")

# å›ºå®šéšæœºç§å­ï¼ˆå¯å¤ç°ç»­è®­ï¼‰
SEED = 2025
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ----------------- è¾“å‡ºè·¯å¾„ -----------------
now = datetime.now()
timestamp = now.strftime("%d-%b-%Y-%H-%M-%S")
model_name = 'SPICER_fastmri'
save_root_tmp = os.path.join(TMPDIR, "spicer_out", f"{model_name}_{timestamp}")
save_root_final = os.path.join(HOME, "spicer_outputs", f"{model_name}_{timestamp}")
os.makedirs(save_root_tmp, exist_ok=True)
os.makedirs(save_root_final, exist_ok=True)
save_root = save_root_tmp

# TensorBoard
log_dir = os.path.join(TMPDIR, "tensorboard_logs", timestamp)
os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir=log_dir)

# ----------------- EarlyStopping -----------------
class EarlyStopping:
    def __init__(self, patience=10, verbose=False):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.verbose = verbose
        self.best_epoch = None

    def __call__(self, val_score, epoch):
        if self.best_score is None:
            self.best_score = val_score
            self.best_epoch = epoch
        elif val_score < self.best_score:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter} / {self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = val_score
            self.best_epoch = epoch
            self.counter = 0

# ----------------- æ•°æ® -----------------
train_idx = range(564, 694)   # 130 subjects
val_idx   = range(694, 709)   # 15 subjects
# test_idx  = range(709, 729)   # 20 subjects ï¼ˆæµ‹è¯•è„šæœ¬ä½¿ç”¨ï¼‰

dataset = RealMeasurement(idx_list=train_idx, acceleration_rate=ACCELERATION,
                          is_return_y_smps_hat=True,
                          mask_pattern='uniformly_cartesian',
                          smps_hat_method='eps')
trainloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)

val_dataset = RealMeasurement(idx_list=val_idx, acceleration_rate=ACCELERATION,
                              is_return_y_smps_hat=True,
                              mask_pattern='uniformly_cartesian',
                              smps_hat_method='eps')
valloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)

# ----------------- æ¨¡å‹/ä¼˜åŒ–å™¨/è°ƒåº¦å™¨ -----------------
model = SPNet(num_cascades=8, pools=4, chans=18, sens_pools=4, sens_chans=8).to(device)

# ä¿å­˜â€œéšæœºåˆå§‹åŒ–â€æƒé‡ï¼Œæµ‹è¯•æ—¶å¯ç”¨æ¥ç”Ÿæˆâ€œåˆå§‹åŒ–è¾“å‡ºâ€
init_weight_path = os.path.join(save_root, "init_model.pth")
torch.save(model.state_dict(), init_weight_path)
print(f"[INIT] Saved random-initialized weights -> {init_weight_path}")

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0)
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)

# ----------------- è®­ç»ƒæ—¥å¿— -----------------
snr_best = []
train_loss_log, val_loss_log = [], []
train_psnr_log, val_psnr_log = [], []
train_ssim_log, val_ssim_log = [], []
best_model_state = None
best_psnr = -1e9

# ----------------- æ–­ç‚¹ä¿å­˜/æ¢å¤ï¼ˆå¢å¼ºï¼‰ -----------------
def save_checkpoint_full(epoch, tag="last"):
    ckpt = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_loss_log': train_loss_log,
        'val_loss_log': val_loss_log,
        'train_psnr_log': train_psnr_log,
        'val_psnr_log': val_psnr_log,
        'train_ssim_log': train_ssim_log,
        'val_ssim_log': val_ssim_log,
        'rng': {
            'python': random.getstate(),
            'numpy': np.random.get_state(),
            'torch': torch.get_rng_state(),
            'torch_cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,
        },
        'meta': {
            'acceleration': ACCELERATION,
        }
    }
    path = os.path.join(save_root, f"checkpoint_{tag}.pth")
    torch.save(ckpt, path)
    if tag == "last":
        torch.save(ckpt, os.path.join(save_root, "checkpoint_last.pth"))
    print(f"[CKPT] saved -> {path}")

def try_resume():
    # 1) æ˜¾å¼ --resume_path
    if args.resume and args.resume_path and os.path.exists(args.resume_path):
        path = args.resume_path
    # 2) æŒ‡å®šç›®å½•ä¸‹çš„ checkpoint_last.pth
    elif args.resume_last_dir and os.path.exists(os.path.join(args.resume_last_dir, "checkpoint_last.pth")):
        path = os.path.join(args.resume_last_dir, "checkpoint_last.pth")
    # 3) å½“å‰ save_root ä¸‹çš„ checkpoint_last.pth
    else:
        path = os.path.join(save_root, "checkpoint_last.pth")
        if not os.path.exists(path):
            return 0  # ä¸æ¢å¤

    print(f"ğŸ” æ¢å¤è®­ç»ƒï¼š{path}")
    ckpt = torch.load(path, map_location=device, weights_only=False)
    model.load_state_dict(ckpt['model_state_dict'])
    optimizer.load_state_dict(ckpt['optimizer_state_dict'])
    if 'scheduler_state_dict' in ckpt:
        scheduler.load_state_dict(ckpt['scheduler_state_dict'])

    # æ¢å¤æ—¥å¿—
    train_loss_log[:] = ckpt.get('train_loss_log', [])
    val_loss_log[:]   = ckpt.get('val_loss_log', [])
    train_psnr_log[:] = ckpt.get('train_psnr_log', [])
    val_psnr_log[:]   = ckpt.get('val_psnr_log', [])
    train_ssim_log[:] = ckpt.get('train_ssim_log', [])
    val_ssim_log[:]   = ckpt.get('val_ssim_log', [])

    # æ¢å¤éšæœºæ•°çŠ¶æ€ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
    rng = ckpt.get('rng', {})
    if rng.get('python') is not None:
        random.setstate(rng['python'])
    if rng.get('numpy') is not None:
        np.random.set_state(rng['numpy'])
    if rng.get('torch') is not None:
        torch.set_rng_state(torch.as_tensor(rng['torch']))
    if torch.cuda.is_available() and rng.get('torch_cuda') is not None:
        cuda_states = []
        for st in rng['torch_cuda']:
            cuda_states.append(torch.as_tensor(st, dtype=torch.uint8))
        torch.cuda.set_rng_state_all(cuda_states)

    start_epoch = ckpt.get('epoch', -1) + 1
    print(f"[CKPT] resume start_epoch = {start_epoch}")
    return start_epoch

# æ•è· SLURM SIGTERMï¼Œç´§æ€¥ä¿å­˜
CURRENT_EPOCH = 0
def _handle_sigterm(signum, frame):
    print("[CKPT] Caught SIGTERM, saving checkpoint_last and exiting...")
    save_checkpoint_full(CURRENT_EPOCH, tag="last")
    sys.exit(0)
signal.signal(signal.SIGTERM, _handle_sigterm)

# ----------------- è®­ç»ƒ/éªŒè¯ -----------------
def train(epoch):
    model.train()
    psnrs, losses, ssims = [], [], []
    for iteration, samples in enumerate(tqdm(trainloader, desc=f"Train [{epoch:03d}]")):
        # æ•°æ®è§£åŒ…ï¼šä¸ä½ çš„æ•°æ®é›† RealMeasurement å¯¹é½
        x_hat, smps_hat, y, mask_m, mask_n = samples
        x_hat = x_hat.to(device)
        mask_m = mask_m.byte().to(device)
        mask_n = mask_n.byte().to(device)
        y = y.to(device)
        y_m = y * mask_m
        y_n = y * mask_n

        # squeeze ä»¥é€‚é… model çš„è¾“å…¥
        y_m_input = y_m.squeeze(1) if y_m.shape[1] == 1 else y_m
        y_n_input = y_n.squeeze(1) if y_n.shape[1] == 1 else y_n

        ny = y_m.shape[-2]
        ACS_size = ((ny // 2) - (int(ny * 0.2 * (2 / ACCELERATION)) // 2)) * 2

        output_m, smap_m = model(torch.view_as_real(y_m_input), mask_m, ACS_center=(ny // 2), ACS_size=ACS_size)
        output_n, smap_n = model(torch.view_as_real(y_n_input), mask_n, ACS_center=(ny // 2), ACS_size=ACS_size)

        smap_m_1 = torch.view_as_complex(smap_m.squeeze())
        smap_n_1 = torch.view_as_complex(smap_n.squeeze())

        h_output_n = fmult(torch.view_as_complex(output_m), smap_m_1, mask_n)
        h_output_m = fmult(torch.view_as_complex(output_n), smap_n_1, mask_m)

        # ä¸ä½œè€…å®ç°ä¸€è‡´çš„ SPICER loss
        loss = spicer_loss(h_output_m, y_m, h_output_n, y_n, smap_m, gamma=1.0, tau=0.1)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # PSNR & SSIMï¼ˆå›¾åƒåŸŸç›‘æ§ï¼‰
        output_show = complex_abs(output_m.detach().cpu().squeeze())
        output_show = normlize(output_show)
        target_show = normlize(torch.abs(x_hat.squeeze()))
        psnrs.append(compare_psnr(output_show.to(device), target_show.to(device)).cpu())
        ssims.append(compare_ssim(output_show[None, None].to(device), target_show[None, None].to(device)).cpu())
        losses.append(loss.item())

    train_loss_log.append(float(np.mean(losses)))
    train_psnr_log.append(float(np.mean(psnrs)))
    train_ssim_log.append(float(np.mean(ssims)))

    writer.add_scalar("Loss_Train", train_loss_log[-1], epoch)
    writer.add_scalar("PSNR_Train", train_psnr_log[-1], epoch)
    writer.add_scalar("SSIM_Train", train_ssim_log[-1], epoch)
    print(f"[Train Debug] Epoch {epoch} - Loss: {train_loss_log[-1]:.6f}")

def val(epoch):
    model.eval()
    psnrs, ssims, losses = [], [], []
    with torch.no_grad():
        for iteration, samples in enumerate(valloader):
            # å’Œ train() ä¸€è‡´çš„æ•°æ®è§£åŒ…
            x_hat, smps_hat, y, mask_m, mask_n = samples
            x_hat = x_hat.to(device)
            mask_m = mask_m.byte().to(device)
            mask_n = mask_n.byte().to(device)
            y = y.to(device)
            y_m = y * mask_m
            y_n = y * mask_n

            # squeeze é€‚é…æ¨¡å‹è¾“å…¥
            y_m_input = y_m.squeeze(1) if y_m.shape[1] == 1 else y_m
            y_n_input = y_n.squeeze(1) if y_n.shape[1] == 1 else y_n

            ny = y_m.shape[-2]
            ACS_size = ((ny // 2) - (int(ny * 0.2 * (2 / ACCELERATION)) // 2)) * 2

            # å‰å‘ä¼ æ’­
            output_m, smap_m = model(torch.view_as_real(y_m_input), mask_m, ACS_center=(ny // 2), ACS_size=ACS_size)
            output_n, smap_n = model(torch.view_as_real(y_n_input), mask_n, ACS_center=(ny // 2), ACS_size=ACS_size)

            smap_m_1 = torch.view_as_complex(smap_m.squeeze())
            smap_n_1 = torch.view_as_complex(smap_n.squeeze())

            h_output_n = fmult(torch.view_as_complex(output_m), smap_m_1, mask_n)
            h_output_m = fmult(torch.view_as_complex(output_n), smap_n_1, mask_m)

            # === å…³é”®æ”¹åŠ¨ï¼šéªŒè¯ä¹Ÿç”¨ SPICER loss ===
            loss = spicer_loss(h_output_m, y_m, h_output_n, y_n, smap_m, gamma=1.0, tau=0.1)

            # å›¾åƒåŸŸç›‘æ§æŒ‡æ ‡
            output_show = normlize(complex_abs(output_m.detach().cpu().squeeze())).to(device)
            target_show = normlize(torch.abs(x_hat.squeeze())).to(device)
            psnrs.append(compare_psnr(output_show, target_show).cpu())
            ssims.append(compare_ssim(output_show[None, None], target_show[None, None]).cpu())
            losses.append(loss.item())

    val_psnr_log.append(float(np.mean(psnrs)))
    val_ssim_log.append(float(np.mean(ssims)))
    val_loss_log.append(float(np.mean(losses)))

    writer.add_scalar("Loss_Val", val_loss_log[-1], epoch)
    writer.add_scalar("PSNR_Val", val_psnr_log[-1], epoch)
    writer.add_scalar("SSIM_Val", val_ssim_log[-1], epoch)

    # ä¿å­˜æ¨¡å‹
    if epoch % 5 == 0 or val_psnr_log[-1] >= max(val_psnr_log):
        torch.save(model.state_dict(), os.path.join(save_root, f"N2N_{epoch:03d}.pth"))

    print(f"[Val Debug] Epoch {epoch} - Loss: {val_loss_log[-1]:.6f}, PSNR: {val_psnr_log[-1]:.4f}, SSIM: {val_ssim_log[-1]:.4f}")


# ----------------- ä¸»è®­ç»ƒå¾ªç¯ï¼ˆå«ç»­è®­ & SIGTERM ä¿æŠ¤ï¼‰ -----------------
start_epoch = try_resume()  # è‹¥æ‰¾ä¸åˆ°æ–­ç‚¹åˆ™è¿”å› 0

early_stopper = EarlyStopping(patience=patience, verbose=True)

for epoch in range(start_epoch, epoch_number):
    CURRENT_EPOCH = epoch  # ä¾› SIGTERM handler ä½¿ç”¨
    print(f"\nğŸ” Epoch {epoch}/{epoch_number} å¼€å§‹")
    train(epoch)
    print(f"âœ… Train epoch {epoch} completed. Loss: {train_loss_log[-1]:.4e}")
    val(epoch)
    print(f"âœ… Val   epoch {epoch} completed. Loss: {val_loss_log[-1]:.4e}")

    # åˆ·æ–° TensorBoard
    writer.flush()

    # ä¿å­˜â€œlastâ€æ–­ç‚¹ï¼ˆå…¨é‡ï¼‰
    save_checkpoint_full(epoch, tag="last")

    # ä¿å­˜ best modelï¼ˆä»¥ Val PSNR ä¸ºå‡†ï¼‰
    #if val_psnr_log[-1] > best_psnr:
    #    best_psnr = val_psnr_log[-1]
    #    torch.save(model.state_dict(), os.path.join(save_root, "best_model.pth"))
    if val_psnr_log[-1] > best_psnr:
        best_psnr = val_psnr_log[-1]
        best_epoch = epoch
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'psnr': best_psnr,
        }, os.path.join(save_root, "best_model.pth"))
        print(f"[BEST] Updated best_model at epoch {epoch}, PSNR={best_psnr:.4f}")

    # Early stopping
    early_stopper(val_psnr_log[-1], epoch)
    if early_stopper.early_stop:
        print(f"â›”ï¸ Early stopping triggered at epoch {epoch}, best epoch was {early_stopper.best_epoch}")
        break

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step()
    current_lr = scheduler.get_last_lr()[0]
    print(f"ğŸ”¹ Current learning rate: {current_lr:.6f}")

# ----------------- è®­ç»ƒæ”¶å°¾ï¼šæ›²çº¿ + å¤åˆ¶äº§ç‰© -----------------
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1); plt.plot(train_loss_log, label='Train'); plt.plot(val_loss_log, label='Val'); plt.title("Loss"); plt.legend(); plt.grid()
plt.subplot(1, 3, 2); plt.plot(train_psnr_log, label='Train'); plt.plot(val_psnr_log, label='Val'); plt.title("PSNR"); plt.legend(); plt.grid()
plt.subplot(1, 3, 3); plt.plot(train_ssim_log, label='Train'); plt.plot(val_ssim_log, label='Val'); plt.title("SSIM"); plt.legend(); plt.grid()
plt.tight_layout()
plt.savefig(os.path.join(save_root, "training_curves.png"))

print("\nâœ… æ‹·è´æ¨¡å‹ä¸å›¾åƒåˆ°:", save_root_final)
os.makedirs(save_root_final, exist_ok=True)
for file in os.listdir(save_root):
    src = os.path.join(save_root, file)
    dst = os.path.join(save_root_final, file)
    #if not os.path.exists(dst):
    shutil.copy2(src, dst)
print("âœ… æ¨¡å‹ä¸å›¾åƒå·²å¤åˆ¶å®Œæ¯• âœ…")

metrics_dict = {
    'epoch': list(range(start_epoch, start_epoch + len(train_loss_log))),
    'train_loss': train_loss_log,
    'val_loss': val_loss_log,
    'train_psnr': train_psnr_log,
    'val_psnr': val_psnr_log,
    'train_ssim': train_ssim_log,
    'val_ssim': val_ssim_log,
}
df_metrics = pd.DataFrame(metrics_dict)
df_metrics.to_csv(os.path.join(save_root, "metrics.csv"), index=False)


#import torch

#ckpt = torch.load("spicer_out/.../best_model.pth", map_location="cpu")
#print("Best model at epoch:", ckpt['epoch'])
#print("Val PSNR:", ckpt['psnr'])